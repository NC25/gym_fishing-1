---
output: github_document
---

```{r}
unlink("fishing.csv")
```

```{python results="hide"}
import gym
import gym_fishing
from stable_baselines.common.policies import MlpPolicy
from stable_baselines.common.vec_env import DummyVecEnv
from stable_baselines import PPO2
```

```{python  results="hide"}
env = gym.make('fishing-v0')
env.n_actions = 100
model = PPO2(MlpPolicy, env, verbose=1)
model.learn(total_timesteps=200000)

obs = env.reset()
for i in range(1000):
    action, _states = model.predict(obs)
    obs, rewards, dones, info = env.step(action)
    env.render()

env.close()

```

## 3 discrete states

```{python results="hide"}
env = gym.make('fishing-v0')
env.n_actions = 3
model = PPO2(MlpPolicy, env, verbose=1)
model.learn(total_timesteps=200000)

def driver(trained_model, num_episodes, num_timesteps):
   obs = env.reset()
   y = []
   total_rewards = []
   all_rewards = []
   for i in range(num_episodes):
     action, _states = trained_model.predict(obs)
     obs, rewards, dones, info = env.step(action)
     y.append(action)
     total_rewards.append(rewards)
     all_rewards.append(sum(total_rewards))
   def get_action():
       return y
   def get_reward():
       return all_rewards
   get_action()
   get_reward()
   env.render()
   env.close()


   #def plot_action():
   x = np.linspace(0, num_episodes, num_episodes)
   fig, ax = plt.subplots()  # Create a figure and an axes. 
   ax.scatter(x, get_action())
   ax.set_xlabel('Episodes')
   ax.set_ylabel('Harvest')

    
   #def plot_reward():
   x = np.linspace(0, num_timesteps, num_timesteps)
   fig, bx = plt.subplots()  # Create a figure and an axes.
   bx.plot(x, get_reward(), label='linear')
   bx.set_xlabel('Timesteps')
   ax.set_ylabel('Cumulative Reward')

```





```{r}
library(tidyverse)
fishing <- read_csv("fishing.csv", 
                    col_names = c("time", "state", "harvest", "action"))

d <- max(fishing$time)
n <-dim(fishing)[1] / d

fishing$rep <- as.character(vapply(1:n, rep, integer(d), d))

## Reward is calculated as net (cumulative) reward without any discounting (gamma = 1)
gamma <- 1.0
price <- 1.0
fishing <- fishing %>% 
  group_by(rep) %>% 
  mutate(reward = cumsum(price * harvest * gamma^time))

fishing %>% summarize(max(reward))
```


```{r}
fishing %>% 
#  filter(time < 100) %>%
  ggplot(aes(time, state, col = rep)) + geom_line() + facet_wrap(~rep)

```

```{r}
fishing %>% 
#  filter(time < 150) %>%
  ggplot(aes(time, harvest, col = rep)) + geom_line()  + facet_wrap(~rep)

```

```{r}
fishing %>% 
 # filter(time < 30) %>%
  ggplot(aes(time, action, col = rep)) + geom_point() + geom_line() + facet_wrap(~rep)

```


